#+TITLE: Involving CPUs into Multi-GPU Deep Learning
#+OPTIONS: toc:nil num:nil ^:nil html-postamble:t html-preamble:t
* Links
** [[file:ICPE_2018_paper_60.pdf][Link to the paper]]
** [[https://github.com/tungld/caffe][Public link to the artifact]]

* Getting started
We extended Caffe, an open source deep learning framework, to provide features
proposed in the paper.
Features include:
1. A CPU-based layer-wise reduction method, named CGDP, where gradient
   accumulation is done layer by layer using CPUs.
2. A method to automatically find a suitable chunk of layers for CGDP so that
   the training time is reduced.

The first feature are mainly implemented in the file [[file:src/caffe/parallel.cpp][src/caffe/parallel.cpp]]
(L447 to the end of the file). 
The second feature are mainly implemented in the file [[file:src/caffe/solver.cpp][src/caffe/solver.cpp]]
(L238--L308).

Because we use CPUs for gradient accumulation via OpenMP, please confirm that there is no
process dominated CPUs on host while running experiments.

** Compilation
Follow [[http://caffe.berkeleyvision.org/installation.html][Caffe's installation instructions]] to compile the source code.
In the paper, we used GPUs for training, thus we need to compile Caffe with CUDA and cuDNN.
We used CUDA v8.0.44 and CuDNN v5.1.5.

In general, configure the build by copying and modifying the example ~Makefile.config.icpe18~ for your setup.
#+begin_example
$ cp Makefile.config.icpe18 Makefile.config
# Adjust Makefile.config (e.g. paths to libraries)
$ make all
#+end_example
After compiling the source code, execution files are generated in the directory ~build/tools~.
* Step-by-step instructions
Datasets and neural network models for the paper are put in the directory ~icpe18-artifact~
#+begin_example
$ ls icpe18-artifact
datasets  models
#+end_example
Datasets are just for measuring performance not accuracy.
** Get results for Figure 6(a)
Figure 6(a) showed a comparison between the original caffe and our CGDP method
for three neural networks: AlexNet, GoogLeNet and VGGNets.  Configuration files
for these networks are in subdirectories named after network names in the
directory ~icpe18-artifact/models~

Taking VGGNet as an example, to get the training time for VGGNet with the
original caffe, do as follows:
#+begin_example
$ cd icpe18-artifact
$ ../build/tools/caffe.bin train --solver models/vggnet/solver.prototxt --gpu 0,1,2,3 --bvlc
#+end_example

Here, we used 4 GPUs. The screen will print out information like these:
#+begin_example
I1221 09:57:55.255875 134492 parallel.cpp:433] Starting Optimization
I1221 09:57:55.256130 134492 solver.cpp:365] Solving VGG_ILSVRC_16_layers
I1221 09:57:55.256141 134492 solver.cpp:366] Learning Rate Policy: step
I1221 09:57:55.716061 134492 solver.cpp:257] Iteration 0, loss = 7.17581
I1221 09:57:55.716094 134492 solver.cpp:273]     Train net output #0: loss/loss = 7.17581 (* 1 = 7.17581 loss)
I1221 09:57:55.816109 134492 sgd_solver.cpp:106] Iteration 0, lr = 0.016968
I1221 09:57:55.832825 134492 solver.cpp:322]     Average time: 0.576444 seconds/iteration
I1221 09:58:00.193527 134492 solver.cpp:257] Iteration 10, loss = 87.3365
I1221 09:58:00.193578 134492 solver.cpp:273]     Train net output #0: loss/loss = 87.3365 (* 1 = 87.3365 loss)
I1221 09:58:00.241466 134492 sgd_solver.cpp:106] Iteration 10, lr = 0.016968
I1221 09:58:00.257027 134492 solver.cpp:322]     Average time: 0.442414 seconds/iteration
I1221 09:58:04.618576 134492 solver.cpp:257] Iteration 20, loss = 87.3365
I1221 09:58:04.618621 134492 solver.cpp:273]     Train net output #0: loss/loss = 87.3365 (* 1 = 87.3365 loss)
I1221 09:58:04.666648 134492 sgd_solver.cpp:106] Iteration 20, lr = 0.016968
I1221 09:58:04.682229 134492 solver.cpp:322]     Average time: 0.442513 seconds/iteration
I1221 09:58:09.043898 134492 solver.cpp:257] Iteration 30, loss = 87.3365
I1221 09:58:09.043943 134492 solver.cpp:273]     Train net output #0: loss/loss = 87.3365 (* 1 = 87.3365 loss)
I1221 09:58:09.092217 134492 sgd_solver.cpp:106] Iteration 30, lr = 0.016968
I1221 09:58:09.107780 134492 solver.cpp:322]     Average time: 0.442547 seconds/iteration
I1221 09:58:13.472877 134492 solver.cpp:257] Iteration 40, loss = 87.3365
I1221 09:58:13.472945 134492 solver.cpp:273]     Train net output #0: loss/loss = 87.3365 (* 1 = 87.3365 loss)
I1221 09:58:13.520750 134492 sgd_solver.cpp:106] Iteration 40, lr = 0.016968
I1221 09:58:13.536312 134492 solver.cpp:322]     Average time: 0.442847 seconds/iteration
#+end_example
where we can see how long does it take for one iteration, i.e. ~0.442547 seconds/iteration~.

To get the training time for our CGDP method, just remove the option ~--bvlc~ from the command line:
#+begin_example
$ cd icpe18-artifact
$ ../build/tools/caffe.bin train --solver models/vggnet/solver.prototxt --gpu 0,1,2,3
#+end_example
The screen will print out information like these:
#+begin_example
I1221 10:02:51.489693 134949 parallel.cpp:900] Starting Optimization
I1221 10:02:51.532088 134949 solver.cpp:365] Solving VGG_ILSVRC_16_layers
I1221 10:02:51.532109 134949 solver.cpp:366] Learning Rate Policy: step
I1221 10:02:52.003957 134949 solver.cpp:257] Iteration 0, loss = 7.46468
I1221 10:02:52.004024 134949 solver.cpp:273]     Train net output #0: loss/loss = 7.46468 (* 1 = 7.46468 loss)
I1221 10:02:52.025581 134949 sgd_solver.cpp:106] Iteration 0, lr = 0.016968
I1221 10:02:52.051532 134949 solver.cpp:322]     Average time: 0.519296 seconds/iteration
I1221 10:02:55.631618 134949 solver.cpp:257] Iteration 10, loss = 3.18923
I1221 10:02:55.631703 134949 solver.cpp:273]     Train net output #0: loss/loss = 3.18923 (* 1 = 3.18923 loss)
I1221 10:02:55.633420 134949 sgd_solver.cpp:106] Iteration 10, lr = 0.016968
I1221 10:02:55.645939 134949 solver.cpp:322]     Average time: 0.359434 seconds/iteration
I1221 10:02:59.236099 134949 solver.cpp:257] Iteration 20, loss = 2.48356
I1221 10:02:59.236176 134949 solver.cpp:273]     Train net output #0: loss/loss = 2.48356 (* 1 = 2.48356 loss)
I1221 10:02:59.238724 134949 sgd_solver.cpp:106] Iteration 20, lr = 0.016968
I1221 10:02:59.251245 134949 solver.cpp:322]     Average time: 0.360525 seconds/iteration
I1221 10:03:02.874429 134949 solver.cpp:257] Iteration 30, loss = 2.62243
I1221 10:03:02.874490 134949 solver.cpp:273]     Train net output #0: loss/loss = 2.62243 (* 1 = 2.62243 loss)
I1221 10:03:02.878609 134949 sgd_solver.cpp:106] Iteration 30, lr = 0.016968
I1221 10:03:02.891129 134949 solver.cpp:322]     Average time: 0.363983 seconds/iteration
I1221 10:03:06.519271 134949 solver.cpp:257] Iteration 40, loss = 2.52941
I1221 10:03:06.519323 134949 solver.cpp:273]     Train net output #0: loss/loss = 2.52941 (* 1 = 2.52941 loss)
I1221 10:03:06.524355 134949 sgd_solver.cpp:106] Iteration 40, lr = 0.016968
I1221 10:03:06.536876 134949 solver.cpp:322]     Average time: 0.364569 seconds/iteration
#+end_example
Now the training time is about ~0.363983 seconds/iteration~ that is faster than the original Caffe.

Do the same comparisons for the other networks, AlexNet and Resnet-152, by replacing the value of ~--solver~.
#+begin_example
$ cd icpe18-artifact
# original caffe for AlexNet
$ ../build/tools/caffe.bin train --solver models/bvlc_alexnet/solver.prototxt --gpu 0,1,2,3 --bvlc
# our CGDP method for AlexNet
$ ../build/tools/caffe.bin train --solver models/bvlc_alexnet/solver.prototxt --gpu 0,1,2,3

# original caffe for GoogLeNet
$ ../build/tools/caffe.bin train --solver models/bvlc_googlenet/solver.prototxt --gpu 0,1,2,3 --bvlc
# our CGDP method for GoogLeNet
$ ../build/tools/caffe.bin train --solver models/bvlc_googlenet/solver.prototxt --gpu 0,1,2,3
#+end_example

** Get results for Figure 8
Figure 8 showed results for Resnet-152 using our CGDP method together with a
heuristic algorithm to find a good chunk of layers.

We will get four following results
1. Training Resnet-152 with one GPU.
2. Training Resnet-152 with 4 GPUs and using original Caffe
3. Training Resnet-152 with 4 GPUs and using CGDP
4. Training Resnet-152 with 4 GPUs, using CGDP and the heuristic algorithm with different parameters.

For the first result, invoke the following command:
#+begin_example
$ cd icpe18-artifact
$ ../build/tools/caffe.bin train --solver models/resnet-152/solver.prototxt --gpu 0 --bvlc
#+end_example

For the second result, invoke the following command:
#+begin_example
$ cd icpe18-artifact
$ ../build/tools/caffe.bin train --solver models/resnet-152/solver.prototxt --gpu 0,1,2,3 --bvlc
#+end_example

For the third result, invoke the following command:
#+begin_example
$ cd icpe18-artifact
$ ../build/tools/caffe.bin train --solver models/resnet-152/solver.prototxt --gpu 0,1,2,3
#+end_example

For the fourth result, invoke the following command:
#+begin_example
$ cd icpe18-artifact
$ ../build/tools/caffe.bin train --solver models/resnet-152/solver_lwr_opt.prototxt --gpu 0,1,2,3
#+end_example

Parameters for the heuristic algorithm are configured by modifying the file
~solver_lwr_opt.prototxt~. There are two parameters:
- ~lwr_opt_step~: the parameter ~step~ in the paper
- ~lwr_opt_range~: the parameter ~range~ in the paper.


